import logging as log
from collections import defaultdict

from gensim import corpora, models, similarities

import constants
import file_funcs
import utils

log.basicConfig(format=constants.log_format, level=constants.log_level)

documents = []


def participles(file_content_map):
    # 分词
    texts = []
    for key in file_content_map.keys():
        line = file_content_map[key]['data_list']
        documents.append(line)
        splited_list = line.split(",")
        words = []
        for split in splited_list:
            words += utils.participles(split)
        texts.append(words)
    return texts


def calc_words_frequency(texts):
    # 开始计算词频
    log.info('开始计算词频')
    frequency = defaultdict(int)
    for text in texts:
        for word in text:
            frequency[word] += 1
    texts = [[word for word in text if frequency[word] > 1] for text in texts]
    return texts


def create_dirctionary(texts):
    log.info('创建字典 单词 - 编号')
    # 通过这些文档抽取一个“词袋（bag-of-words)“，将文档的token映射为id：
    dictionary = corpora.Dictionary(texts)
    log.info(dictionary)
    log.info(dictionary.token2id)
    # dictionary.token2id的结果
    # {'': 0, '1081': 1, '10811': 2, '108110': 3, '1081106': 4, '10811069': 5, '1406': 6, '14069': 7, '140692': 8, '1406923': 9, '14069237': 10, '1456': 11, '14569': 12, '145691': 13, '1456912': 14, '14569125': 15, '15DP': 16, '15DPK': 17, '15DPK3': 18, '15DPK36': 19, '15DPK363': 20, '15DPK38': 21, '15DPK380': 22, '0912': 23, '09123': 24, '091234': 25, '0912345': 26, '09123456': 27, '1075': 28, '10754': 29, '15DPK2': 30, '15DPK20': 31, '15DPK200': 32, '15DPK21': 33, '15DPK210': 34, '15DPK26': 35, '15DPK261': 36, '15DPK30': 37, '15DPK300': 38, '15DPK366': 39, '0970': 40, '1230': 41, '12301': 42, '123010': 43, '1230100': 44, '12301000': 45, '1555': 46, '15552': 47, '155520': 48, '1555205': 49, '15552050': 50, '15557': 51, '155570': 52, '1555706': 53, '15557060': 54, '15557061': 55, '1655': 56, '16557': 57, '165570': 58, '1655706': 59, '15JT': 60, '15JT5': 61, '15JT50': 62, '15JT505': 63, '15JT5058': 64, '15JT506': 65, '15JT5060': 66, '1240': 67, '12400': 68, '124001': 69, '1240018': 70, '12400180': 71, '1258': 72, '12588': 73, '125880': 74, '1258807': 75, '12588076': 76, '1355': 77, '13553': 78, '135530': 79, '1355303': 80, '1366': 81, '13660': 82, '136606': 83, '1366061': 84, '1577': 85, '15773': 86, '157731': 87, '1577310': 88, '15773103': 89, '1577311': 90, '15773111': 91, '15777': 92, '157770': 93, '1577705': 94, '15777052': 95, '1191': 96, '11912': 97, '119124': 98, '1191242': 99, '11912424': 100, '1209': 101, '12096': 102, '1218': 103, '12182': 104, '121820': 105, '1218200': 106, '1218201': 107, '12182015': 108, '1218204': 109, '12182041': 110, '121821': 111, '1218213': 112, '12182134': 113, '1218214': 114, '12182141': 115, '15SF': 116, '15SF1': 117, '15SF13': 118, '15SF131': 119, '15SF1316': 120, '1266': 121, '12669': 122, '15TT': 123, '0643': 124, '06431': 125, '064310': 126, '0643100': 127, '06431001': 128, '0803': 129, '08030': 130, '09YT': 131, '09YT2': 132, '09YT21': 133, '09YT216': 134, '09YT2162': 135, '09YT5': 136, '09YT56': 137, '09YT568': 138, '1200': 139, '12000': 140, '120000': 141, '1200000': 142, '12000003': 143, '1269': 144, '12693': 145, '1270': 146, '12700': 147, '127006': 148, '1270060': 149, '12700601': 150, '12D2': 151, '12D20': 152, '12D200': 153, '12D2000': 154, '12D20001': 155, '12FJ': 156, '12FJ2': 157, '12FJ21': 158, '12FJ210': 159, '12FJ2108': 160, '12G0': 161, '12G00': 162, '12G002': 163, '12G0024': 164, '12G00240': 165, '12G01': 166, '12G011': 167, '12G0118': 168, '12G01180': 169, '12G1': 170, '12G10': 171, '12G101': 172, '12G1012': 173, '12G10123': 174, '12G2': 175, '12G20': 176, '12G202': 177, '12G2020': 178, '12G20200': 179, '12G4': 180, '12G40': 181, '12G401': 182, '12G4013': 183, '12G40130': 184, '12Z0': 185, '12Z00': 186, '12Z000': 187, '12Z0000': 188, '12Z00003': 189, '12Z1': 190, '12Z10': 191, '12Z100': 192, '12Z1005': 193, '12Z10052': 194, '13YT': 195, '13YT5': 196, '13YT56': 197, '13YT568': 198, '1420': 199, '14202': 200, '142021': 201, '1420210': 202, '14202107': 203, '1474': 204, '14741': 205, '147410': 206, '1474100': 207, '14741001': 208, '14DD': 209, '14DD2': 210, '14DD20': 211, '14DD202': 212, '14DD2021': 213, '14YT': 214, '14YT5': 215, '14YT56': 216, '14YT568': 217, '14YT5683': 218, '15BC': 219, '15BC2': 220, '15BC21': 221, '15BC210': 222, '15BC2108': 223, '15YT': 224, '15YT2': 225, '15YT5': 226, '15YT56': 227, '15YT568': 228, '15YT5682': 229, '15YT5683': 230, '15YT5685': 231, '15YT57': 232, '15YT570': 233, '15YT571': 234, '15YT5710': 235, '15YT9': 236, '15YT96': 237, '15YT968': 238, '15YT9685': 239, '15yt': 240, '15yt3': 241, '15yt32': 242, '15yt320': 243, '15yt3209': 244, '15yt9': 245, '15yt97': 246, '16BC': 247, '16BC2': 248, '16BC20': 249, '16BC202': 250, '16BC2021': 251, '16YT': 252, '16YT5': 253, '16YT56': 254, '16YT568': 255, '18G8': 256, '18G80': 257, '18G800': 258, '18G8000': 259, '18G80006': 260, '18YT': 261, '18YT2': 262, '18YT21': 263, '18YT216': 264, '18YT2162': 265, '18YT5': 266, '18YT56': 267, '18YT568': 268, '19R0': 269, '19R02': 270, '19R02Z': 271, '19R02ZY': 272, '19R02ZYT': 273, '1319': 274, '13190': 275, '131901': 276, '1319019': 277, '13190199': 278, '1320': 279, '13209': 280, '132090': 281, '1320900': 282, '13209000': 283, '1331': 284, '13312': 285, '133120': 286, '1331204': 287, '13312043': 288, '1337': 289, '13378': 290, '133788': 291, '1337882': 292, '13378824': 293, '1342': 294, '13428': 295, '134280': 296, '1342802': 297, '1343': 298, '13431': 299, '134316': 300, '1343164': 301, '1346': 302, '13460': 303, '134609': 304, '1346099': 305, '13460990': 306, '13460992': 307, '1353': 308, '13530': 309, '135301': 310, '1353011': 311, '1357': 312, '13570': 313, '135700': 314, '1357001': 315, '13570010': 316, '1360': 317, '13609': 318, '136090': 319, '1360900': 320, '13609000': 321, '1368': 322, '13680': 323, '1369': 324, '1377': 325, '13779': 326, '137790': 327, '1377900': 328, '13779000': 329, '1380': 330, '13800': 331, '138000': 332, '1380000': 333, '1531': 334, '15312': 335, '153120': 336, '1531205': 337, '15312055': 338, '1543': 339, '15432': 340, '154320': 341, '1543200': 342, '15432006': 343, '15432007': 344, '15432008': 345, '15432009': 346, '1264': 347, '12640': 348, '126401': 349, '1272': 350, '12720': 351, '127200': 352, '1276': 353, '1473': 354, '14731': 355, '147316': 356, '1473160': 357, '14731608': 358, '14741000': 359, '1475': 360, '14754': 361, '147547': 362, '1475475': 363, '14754753': 364, '147549': 365, '1475492': 366, '14754924': 367, '14754927': 368, '1475493': 369, '14754932': 370, '1477': 371, '14771': 372, '147711': 373, '1477112': 374, '14771128': 375, '1478': 376, '14782': 377, '147821': 378, '1478219': 379, '147822': 380, '1478220': 381, '147712': 382, '1477121': 383, '14771210': 384, '1220': 385, '12202': 386, '122021': 387, '1220211': 388, '12202117': 389}

    log.info('开始建立语料')
    # 把文档转换为二元组的向量
    # 语料库是一组向量，向量中的元素是一个二元组（编号、频次数），对应分词后的文档中的每一个词。
    # 然后就可以将用字符串表示的文档转换为用id表示的文档向量：
    corpus = [dictionary.doc2bow(text) for text in texts]
    log.info(corpus)
    # [[(0, 5), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 4), (7, 4), (8, 4), (9, 3), (10, 3), (11, 2), (12, 2), (13, 2), (14, 2), (15, 2), (16, 16), (17, 16), (18, 16), (19, 13), (20, 13), (21, 3), (22, 3)], [(0, 5), (16, 16), (17, 16), (18, 5), (19, 2), (23, 2), (24, 2), (25, 2), (26, 2), (27, 2), (28, 2), (29, 2), (30, 11), (31, 3), (32, 3), (33, 6), (34, 6), (35, 2), (36, 2), (37, 2), (38, 2), (39, 2)], [(0, 5), (40, 2), (41, 7), (42, 7), (43, 7), (44, 7), (45, 7), (46, 18), (47, 6), (48, 6), (49, 5), (50, 4), (51, 12), (52, 12), (53, 10), (54, 3), (55, 7), (56, 2), (57, 2), (58, 2), (59, 2)], [(0, 5), (60, 10), (61, 10), (62, 10), (63, 3), (64, 2), (65, 5), (66, 5)], [(0, 5), (67, 3), (68, 3), (69, 3), (70, 2), (71, 2), (72, 3), (73, 3), (74, 3), (75, 2), (76, 2), (77, 3), (78, 3), (79, 3), (80, 3), (81, 3), (82, 3), (83, 3), (84, 2), (85, 20), (86, 11), (87, 11), (88, 4), (89, 2), (90, 7), (91, 6), (92, 9), (93, 9), (94, 9), (95, 9)], [(0, 5), (96, 20), (97, 20), (98, 20), (99, 20), (100, 20), (101, 3), (102, 2), (103, 14), (104, 14), (105, 10), (106, 2), (107, 5), (108, 5), (109, 2), (110, 2), (111, 4), (112, 2), (113, 2), (114, 2), (115, 2), (116, 3), (117, 3), (118, 2), (119, 2), (120, 2)], [(0, 5), (121, 2), (122, 2), (123, 2)], [(0, 5), (124, 2), (125, 2), (126, 2), (127, 2), (128, 2), (129, 2), (130, 2), (131, 5), (132, 2), (133, 2), (134, 2), (135, 2), (136, 3), (137, 3), (138, 2), (139, 2), (140, 2), (141, 2), (142, 2), (143, 2), (144, 3), (145, 2), (146, 4), (147, 4), (148, 4), (149, 4), (150, 3), (151, 6), (152, 6), (153, 6), (154, 6), (155, 6), (156, 3), (157, 3), (158, 3), (159, 3), (160, 3), (161, 6), (162, 3), (163, 3), (164, 2), (165, 2), (166, 3), (167, 3), (168, 2), (169, 2), (170, 2), (171, 2), (172, 2), (173, 2), (174, 2), (175, 4), (176, 4), (177, 4), (178, 4), (179, 4), (180, 3), (181, 3), (182, 3), (183, 3), (184, 3), (185, 3), (186, 3), (187, 3), (188, 3), (189, 3), (190, 4), (191, 4), (192, 4), (193, 4), (194, 4), (195, 4), (196, 4), (197, 4), (198, 4), (199, 3), (200, 3), (201, 3), (202, 3), (203, 2), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 2), (210, 2), (211, 2), (212, 2), (213, 2), (214, 4), (215, 4), (216, 4), (217, 4), (218, 3), (219, 2), (220, 2), (221, 2), (222, 2), (223, 2), (224, 34), (225, 2), (226, 26), (227, 17), (228, 16), (229, 4), (230, 5), (231, 5), (232, 8), (233, 2), (234, 6), (235, 4), (236, 5), (237, 4), (238, 3), (239, 3), (240, 4), (241, 2), (242, 2), (243, 2), (244, 2), (245, 2), (246, 2), (247, 2), (248, 2), (249, 2), (250, 2), (251, 2), (252, 2), (253, 2), (254, 2), (255, 2), (256, 5), (257, 5), (258, 5), (259, 5), (260, 5), (261, 5), (262, 3), (263, 3), (264, 3), (265, 2), (266, 2), (267, 2), (268, 2), (269, 4), (270, 4), (271, 3), (272, 3), (273, 3)], [(0, 5), (274, 2), (275, 2), (276, 2), (277, 2), (278, 2), (279, 2), (280, 2), (281, 2), (282, 2), (283, 2), (284, 3), (285, 2), (286, 2), (287, 2), (288, 2), (289, 16), (290, 16), (291, 16), (292, 16), (293, 16), (294, 2), (295, 2), (296, 2), (297, 2), (298, 4), (299, 4), (300, 4), (301, 3), (302, 9), (303, 9), (304, 9), (305, 8), (306, 2), (307, 3), (308, 2), (309, 2), (310, 2), (311, 2), (312, 2), (313, 2), (314, 2), (315, 2), (316, 2), (317, 3), (318, 3), (319, 3), (320, 3), (321, 3), (322, 2), (323, 2), (324, 3), (325, 3), (326, 2), (327, 2), (328, 2), (329, 2), (330, 2), (331, 2), (332, 2), (333, 2), (334, 3), (335, 3), (336, 3), (337, 3), (338, 3), (339, 14), (340, 14), (341, 14), (342, 14), (343, 3), (344, 2), (345, 3), (346, 3)], [(0, 5), (204, 14), (205, 14), (206, 14), (207, 14), (208, 9), (347, 3), (348, 3), (349, 2), (350, 3), (351, 3), (352, 3), (353, 2), (354, 4), (355, 4), (356, 3), (357, 3), (358, 2), (359, 5), (360, 15), (361, 15), (362, 3), (363, 3), (364, 3), (365, 10), (366, 6), (367, 2), (368, 2), (369, 4), (370, 3), (371, 3), (372, 3), (373, 3), (374, 3), (375, 3), (376, 8), (377, 8), (378, 5), (379, 2), (380, 3), (381, 3)], [(0, 5), (371, 3), (372, 3), (382, 3), (383, 3), (384, 3)], [(0, 5), (385, 18), (386, 18), (387, 18), (388, 18), (389, 18)]]
    # 例如(3, 2)这个元素代表第1篇文档中id为3的单词'108110'出现了2次。
    # 有了这些信息，我们就可以基于这些“训练文档”计算一个TF-IDF“模型”：
    log.info('model init...')

    # 相似度分析
    # 使用TF-IDF模型对语料库建模
    tfidf = models.TfidfModel(corpus)
    # tfidf = models.LsiModel(corpus)
    # tfidf = models.LdaModel(corpus)

    corpus_tfidf = tfidf[corpus]
    for doc in corpus_tfidf:
        log.info(doc)

    log.info('create index...')
    # similarities.MatrixSimilarity类仅仅适合能将所有的向量都在内存中的情况。
    # 例如，如果一个百万文档级的语料库使用该类，可能需要2G内存与256维LSI空间。
    # 如果没有足够的内存，你可以使用similarities.Similarity类。该类的操作只需要固定大小的内存，
    # 因为他将索引切分为多个文件（称为碎片）存储到硬盘上了。
    # 它实际上使用了similarities.MatrixSimilarity和similarities.SparseMatrixSimilarity两个类，
    # 因此它也是比较快的，虽然看起来更加复杂了。
    # 建索引
    index = similarities.MatrixSimilarity(corpus_tfidf)
    return dictionary, index, tfidf


def main():
    file_content_map = file_funcs.read_data_set_from_file()
    # 分词
    texts = participles(file_content_map)
    log.info("分词结果")
    log.info(texts)
    # 计算词频
    texts = calc_words_frequency(texts)
    log.info("计算词频结果")
    log.info(texts)
    dictionary, index, tfidf = create_dirctionary(texts)
    file_funcs.persistence(file_content_map, dictionary, index, tfidf, documents)
    log.warning("重新加载数据源完成")


if __name__ == '__main__':
    main()
